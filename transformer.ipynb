{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../transformer/\")\n",
    "from tqdm import tqdm\n",
    "from data_load import get_batch_data, load_de_vocab, load_en_vocab\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from data_load import load_test_data \n",
    "import os\n",
    "import codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams(object):\n",
    "    #data\n",
    "    source_train = 'corpora/train.tags.de-en.de'\n",
    "    target_train = 'corpora/train.tags.de-en.en'\n",
    "    source_test = 'corpora/IWSLT16.TED.tst2014.de-en.de.xml'\n",
    "    target_test = 'corpora/IWSLT16.TED.tst2014.de-en.en.xml'\n",
    "    \n",
    "    # training\n",
    "    batch_size=32\n",
    "    lr = 0.001\n",
    "    logdir = 'logdir' # log directory\n",
    "    #model\n",
    "    maxlen = 10 # Maximum number of words in a sentence. alias = T.\n",
    "                # Feel free to increase this if you are ambitious.\n",
    "    min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "    hidden_units = 512 # alias = C\n",
    "    num_blocks = 6 # number of encoder/decoder blocks\n",
    "    num_epochs = 1\n",
    "    num_heads = 8\n",
    "    units_per_head=1024\n",
    "    dropout_rate = 0.1\n",
    "    is_training=True\n",
    "    num_layers=6\n",
    "hp=Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(query,key,value,seq_len,units_per_head=1024,num_heads=8,mask=False,scope=\"multihead_attention\", \n",
    "reuse=None):\n",
    "    #使用时一般K和V一样\n",
    "    #query(N,Tq/h,C) key=value(N,Tk/h,C)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "#         units_per_head = query.shape[-1]\n",
    "        \n",
    "        num_units = units_per_head\n",
    "        \n",
    "        #做线性映射，直接um_units*num_heads宽度与num_heads个um_units线性映射是一样的\n",
    "        Q=tf.layers.dense(query,num_units*num_heads,activation=tf.nn.relu)# (N, T_q, C)\n",
    "        K=tf.layers.dense(key,num_units*num_heads,activation=tf.nn.relu)# (N, T_k, C)\n",
    "        V=tf.layers.dense(value,num_units*num_heads,activation=tf.nn.relu)# (N, T_k, C)\n",
    "        #tf.split(dimension, num_split, input)：dimension的意思就是输入张量的哪一个维度，\n",
    "        #如果是0就表示对第0维度进行切割。num_split就是切割的数量\n",
    "        Q=tf.concat(tf.split(Q,num_heads,axis=2),axis=0)#(h*N,Tq,C/h)\n",
    "        K=tf.concat(tf.split(K,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "        V=tf.concat(tf.split(V,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = tf.matmul(Q,tf.transpose(K,[0,2,1]))#(h*N,Tq,Tk)\n",
    "        A = A /(K.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # Key Masking\n",
    "        #将元素中最后一个为度之和全为0的位置标记为0 source(N, T_k,word_dim)\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(key, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        #由于每个queries都要对应这些keys，而mask的key对每个queries都是mask的\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, Q.shape[1], 1]) # (h*N, T_q, T_k)\n",
    "        #定义一个和outputs同shape的paddings，该tensor每个值都设定的极小。用where函数比较，当对应位置的key_masks值为0\n",
    "        #也就是需要mask时，outputs的该值（attention score）设置为极小的值（利用paddings实现），否则保留原来的outputs值。\n",
    "        paddings = tf.ones_like(A)*(-2**32+1)\n",
    "        A = tf.where(tf.equal(key_masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "\n",
    "        #Causality Future blinding\n",
    "        if mask:\n",
    "            #首先定义一个和outputs后两维的shape相同shape（T_q,T_k）的一个张量（矩阵）。\n",
    "            #然后将该矩阵转为三角阵tril。三角阵中，对于每一个T_q,凡是那些大于它角标的T_k值全都为0，\n",
    "            #这样作为mask就可以让query只取它之前的key（self attention中query即key）。\n",
    "            #由于该规律适用于所有query，接下来仍用tile扩展堆叠其第一个维度，构成masks，shape为(h*N, T_q,T_k).\n",
    "            diag_vals = tf.ones_like(A[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(A)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            A = tf.where(tf.equal(masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "        A = tf.nn.softmax(A)# (h*N, T_q, T_k)\n",
    "\n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(query, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(key)[1]]) # (h*N, T_q, T_k)\n",
    "\n",
    "        A *= query_masks # broadcasting. (N, T_q, C)\n",
    "        A=tf.layers.dropout(A, rate=hp.dropout_rate)\n",
    "\n",
    "        # weighted sum\n",
    "        outputs=tf.matmul(A,V)# ( h*N, T_q, C/h)\n",
    "\n",
    "        #Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "        #将形状变回原来形状\n",
    "        outputs = tf.layers.dense(outputs,query.shape[-1])\n",
    "        # Residual connection\n",
    "        outputs += query\n",
    "\n",
    "        #Normalize\n",
    "        outputs=layer_normal(outputs)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs,num_units=[2048,512],reuse=None):\n",
    "    outputs= tf.layers.conv1d(\n",
    "        inputs,filters=num_units[0],kernel_size=1,activation=tf.nn.relu,use_bias=True)\n",
    "    outputs=tf.layers.conv1d(\n",
    "        outputs,filters=num_units[1],kernel_size=1,activation=None,use_bias=True)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    outputs=layer_normal(outputs)\n",
    "    return outputs\n",
    "def layer_normal(inputs,epsilon = 1e-8):\n",
    "    mean,variance = tf.nn.moments(inputs,[-1],keep_dims=True)\n",
    "    params_shape = inputs.shape[-1:]\n",
    "    beta= tf.Variable(tf.zeros(params_shape))\n",
    "    gamma = tf.Variable(tf.ones(params_shape))\n",
    "    normalized = (inputs-mean)/(((variance + epsilon) ** (.5)))\n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "def encoder (pre_enc,num_layers,hparams,scope,seq_len):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"encoder_layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            outputs = multihead_attention(\n",
    "                query=pre_enc,\n",
    "                key=pre_enc,\n",
    "                value=pre_enc,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=False,\n",
    "                seq_len=seq_len)\n",
    "            outputs = feedforward(outputs, num_units=[4*hparams.hidden_units, hparams.hidden_units])\n",
    "    return outputs\n",
    "def decoder (enc,pre_dec,num_layers,hparams,scope,seq_len):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"decoder_layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            output=multihead_attention(\n",
    "                query=pre_dec,\n",
    "                key=pre_dec,\n",
    "                value=pre_dec,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=True,\n",
    "                scope=\"self_attention\",\n",
    "                seq_len=seq_len)\n",
    "            output=multihead_attention(\n",
    "                query=pre_dec,\n",
    "                key=output,\n",
    "                value=output,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=False,\n",
    "                scope=\"vanilla_attention\",\n",
    "                seq_len=seq_len)\n",
    "            output = feedforward(output, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "    return output\n",
    "def position_embedding(inputs,position_size):\n",
    "    batch_size,seq_len = tf.shape(inputs)[0],tf.shape(inputs)[1]\n",
    "    pos_j = 1. / tf.pow(10000., 2 * tf.range(position_size / 2, dtype=tf.float32) / position_size)\n",
    "    pos_j = tf.expand_dims(pos_j, 0)\n",
    "    pos_i = tf.range(tf.cast(seq_len, tf.float32), dtype=tf.float32)\n",
    "    pos_i = tf.expand_dims(pos_i, 1)\n",
    "    pos_ij = tf.matmul(pos_i, pos_j)\n",
    "    pos_ij = tf.concat([tf.cos(pos_ij), tf.sin(pos_ij)], 1)\n",
    "    position_embedding = tf.expand_dims(pos_ij, 0) + tf.zeros((batch_size, seq_len, position_size))\n",
    "    return position_embedding\n",
    "def word_embedding(inputs,vocab_size,num_units,zero_pad=True,scale=True,scope=\"embedding\",reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs,epsilon=0.1):\n",
    "    \"\"\"Applies label smoothing. See https://arxiv.org/abs/1512.00567.\"\"\"\n",
    "    K = tf.to_float(tf.shape(inputs)[-1]) # number of channels\n",
    "    return ((1.-epsilon)*inputs+(epsilon/K))\n",
    "#     return ((1-epsilon)*inputs+(epsilon/K))\n",
    "\n",
    "# # https://github.com/vahidk/EffectiveTensorflow#beam_search\n",
    "# def beamDecoder(enc,dec,hp.num_layers,hp,seq_len,scope=\"beam_decoder\"):\n",
    "#     enc_output=tf.tile(tf.expand_dims(enc, axis=1),multiples=[1, hp.beam_width, 1])\n",
    "#     seq_len=tf.tile(tf.expand_dims(enc_output, axis=1),multiples=[1, hp.beam_width, 1])\n",
    "#     ids = tf.tile([[2]], [hp.batch_size, hp.beam_width])\n",
    "#     sel_ids = tf.zeros([hp.batch_size, hp.beam_width, 0], dtype=ids.dtype)\n",
    "#     mask = tf.ones([batch_size, beam_width], dtype=tf.float32)\n",
    "#     for i in range(hp.maxlen):\n",
    "#         with tf.variable_scope(scope,reuse=True if i>1 else None):\n",
    "#             state, logits = update_fn(state, ids)\n",
    "#             logits = tf.nn.log_softmax(logits)\n",
    "#             sum_logprobs = (tf.expand_dims(sel_sum_logprobs, axis=2) + (logits * tf.expand_dims(mask, axis=2)))\n",
    "#             num_classes = logits.shape.as_list()[-1]\n",
    "#             sel_sum_logprobs, indices = tf.nn.top_k(\n",
    "#                   tf.reshape(sum_logprobs, [batch_size, num_classes * beam_width]),\n",
    "#                   k=beam_width)\n",
    "#             ids = indices % num_classes\n",
    "#             beam_ids = indices // num_classes\n",
    "#             state = batch_gather(state, beam_ids)\n",
    "#             sel_ids = tf.concat([batch_gather(sel_ids, beam_ids),\n",
    "#                        tf.expand_dims(ids, axis=2)], axis=2)\n",
    "#             mask = (batch_gather(mask, beam_ids) *\n",
    "#                     tf.to_float(tf.not_equal(ids, end_token_id)))\n",
    "#     return sel_ids, sel_sum_logprobs    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Tile_2:0\", shape=(3, 2, 3), dtype=float32)\n",
      "Tensor(\"Tile_3:0\", shape=(2, 2), dtype=int32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable beam_decoder/decoder_layer_0/self_attention/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-2da132dbccdc>\", line 11, in multihead_attention\n    Q=tf.layers.dense(query,num_units*num_heads,activation=tf.nn.relu)# (N, T_q, C)\n  File \"<ipython-input-3-2da132dbccdc>\", line 112, in decoder\n    seq_len=seq_len)\n  File \"<ipython-input-4-457eb8131b02>\", line 58, in update_fn\n    state = decoder(state,ids,hp.num_layers,hp,\"decoder\",hp.maxlen)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-21a572be0a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m#     print(tf.log(x).eval())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeamDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"beam_decoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-21a572be0a86>\u001b[0m in \u001b[0;36mbeamDecoder\u001b[0;34m(enc, dec, num_layers, hp, seq_len, scope)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             sum_logprobs = (\n",
      "\u001b[0;32m<ipython-input-5-21a572be0a86>\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(state, ids)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"decoder\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2da132dbccdc>\u001b[0m in \u001b[0;36mdecoder\u001b[0;34m(enc, pre_dec, num_layers, hparams, scope, seq_len)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"self_attention\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 seq_len=seq_len)\n\u001b[0m\u001b[1;32m    113\u001b[0m             output=multihead_attention(\n\u001b[1;32m    114\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-2da132dbccdc>\u001b[0m in \u001b[0;36mmultihead_attention\u001b[0;34m(query, key, value, seq_len, units_per_head, num_heads, mask, scope, reuse)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#做线性映射，直接um_units*num_heads宽度与num_heads个um_units线性映射是一样的\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (N, T_q, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (N, T_k, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# (N, T_k, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 250\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \"\"\"\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    135\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                                     trainable=True)\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m       self.bias = self.add_variable('bias',\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                    trainable=trainable and self.trainable)\n\u001b[0m\u001b[1;32m    459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    740\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 742\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable beam_decoder/decoder_layer_0/self_attention/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-2da132dbccdc>\", line 11, in multihead_attention\n    Q=tf.layers.dense(query,num_units*num_heads,activation=tf.nn.relu)# (N, T_q, C)\n  File \"<ipython-input-3-2da132dbccdc>\", line 112, in decoder\n    seq_len=seq_len)\n  File \"<ipython-input-4-457eb8131b02>\", line 58, in update_fn\n    state = decoder(state,ids,hp.num_layers,hp,\"decoder\",hp.maxlen)\n"
     ]
    }
   ],
   "source": [
    "class Hyperparams(object):\n",
    "    #data\n",
    "    source_train = 'corpora/train.tags.de-en.de'\n",
    "    target_train = 'corpora/train.tags.de-en.en'\n",
    "    source_test = 'corpora/IWSLT16.TED.tst2014.de-en.de.xml'\n",
    "    target_test = 'corpora/IWSLT16.TED.tst2014.de-en.en.xml'\n",
    "    \n",
    "    # training\n",
    "    batch_size=2\n",
    "    lr = 0.001\n",
    "    logdir = 'logdir' # log directory\n",
    "    #model\n",
    "    maxlen = 2 # Maximum number of words in a sentence. alias = T.\n",
    "                # Feel free to increase this if you are ambitious.\n",
    "    min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "    hidden_units = 3 #512 alias = C\n",
    "    num_blocks = 6 # number of encoder/decoder blocks\n",
    "    num_epochs = 1\n",
    "    num_heads = 1\n",
    "    units_per_head=1024\n",
    "    dropout_rate = 0.1\n",
    "    is_training=True\n",
    "    num_layers=1\n",
    "    beam_width=2\n",
    "hp=Hyperparams()\n",
    "import tensorflow as tf\n",
    "enc=tf.Variable([[[1.,1.,1.],[2.,2.,2.],[3.,3.,3.]],[[3.,3.,3.],[4.,4.,4.],[3.,3.,3.]]])\n",
    "dec=tf.Variable([[[0.,0.,0.],[0.,0.,0.],[3.,3.,3.]],[[1.,0.,0.],[0.,0.,0.],[3.,3.,3.]]])\n",
    "seq_len =tf.Variable( [[2],[2]])\n",
    "# https://github.com/vahidk/EffectiveTensorflow#beam_search\n",
    "def get_shape(tensor):\n",
    "    \"\"\"Returns static shape if available and dynamic shape otherwise.\"\"\"\n",
    "    static_shape = tensor.shape.as_list()\n",
    "    dynamic_shape = tf.unstack(tf.shape(tensor))\n",
    "    dims = [s[1] if s[0] is None else s[0]\n",
    "          for s in zip(static_shape, dynamic_shape)]\n",
    "    return dims\n",
    "def batch_gather(tensor, indices):\n",
    "    \"\"\"Gather in batch from a tensor of arbitrary size.\n",
    "\n",
    "    In pseudocode this module will produce the following:\n",
    "    output[i] = tf.gather(tensor[i], indices[i])\n",
    "\n",
    "    Args:\n",
    "    tensor: Tensor of arbitrary size.\n",
    "    indices: Vector of indices.\n",
    "    Returns:\n",
    "    output: A tensor of gathered values.\n",
    "    \"\"\"\n",
    "    shape = get_shape(tensor)\n",
    "    flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n",
    "    indices = tf.convert_to_tensor(indices)\n",
    "    offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)\n",
    "    offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)\n",
    "    output = tf.gather(flat_first, indices + offset)\n",
    "    return output\n",
    "def update_fn(state, ids):\n",
    "    state = decoder (state,pre_dec,num_layers,hparams,scope,seq_len):\n",
    "def beamDecoder(enc,dec_seq,num_layers,hp,seq_len):\n",
    "    enc = tf.contrib.seq2seq.tile_batch(enc,hp.beam_width)\n",
    "    dec_seq = tf.contribi.seq2seq.tile_batch(dec_seq,hp.beam_width)\n",
    "    \n",
    "    dec = word_embedding(dec_seq, vocab_size=len(en2idx), \n",
    "                              num_units=hp.hidden_units,scale=True,scope=\"dec_embed\")\n",
    "    dec += position_embedding(self.x,hp.hidden_units)\n",
    "    begin_token_id=2\n",
    "    end_token_id=1\n",
    "\n",
    "    ids = tf.tile([[begin_token_id]], [batch_size, beam_width])\n",
    "    sel_ids = tf.zeros([batch_size, beam_width, 0], dtype=ids.dtype)\n",
    "    mask = tf.ones([batch_size, beam_width], dtype=tf.float32)\n",
    "    \n",
    "#     paths_ids = \n",
    "    for i in range(hp.maxlen):\n",
    "        with tf.variable_scope(name, reuse=True if i > 0 else None):\n",
    "            dec = decoder(enc,dec,num_layers,hparams,scope,seq_len):\n",
    "    return sel_ids, sel_sum_logprobs\n",
    "x=tf.Variable([[1.]+[0.]*9])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print(tf.log(x).eval())\n",
    "    a,b=beamDecoder(enc,dec,hp.num_layers,hp,seq_len,scope=\"beam_decoder\")\n",
    "    print(a.eval())\n",
    "    print(b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(object):\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if hp.is_training:\n",
    "                self.x,self.y,self.num_batch = get_batch_data() # (N, T)\n",
    "            else: #inference\n",
    "                self.x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "                self.y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "                \n",
    "            de2idx, idx2de = load_de_vocab()\n",
    "            en2idx, idx2en = load_en_vocab()\n",
    "            #[N,T,hp.hidden_units]\n",
    "            self.enc = word_embedding(self.x,vocab_size=len(de2idx),num_units=hp.hidden_units,scale=True,scope=\"enc_embed\")\n",
    "            #[N,T,hp.hidden_units]??\n",
    "            self.enc+= position_embedding(self.x,hp.hidden_units)\n",
    "            self.enc = tf.layers.dropout(self.enc, rate=hp.dropout_rate,training=tf.convert_to_tensor(hp.is_training))\n",
    "            seq_len=hp.maxlen\n",
    "            self.enc = encoder(self.enc,hp.num_layers,hp,\"encoder\",seq_len)\n",
    "            \n",
    "            self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 在开头添加2:<S>\n",
    "            \n",
    "                   \n",
    "            if hp.is_training:\n",
    "                self.dec = word_embedding(self.decoder_inputs, vocab_size=len(en2idx), \n",
    "                                          num_units=hp.hidden_units,scale=True,scope=\"dec_embed\")\n",
    "                self.dec += position_embedding(self.x,hp.hidden_units)\n",
    "                self.dec = tf.layers.dropout(self.dec, rate=hp.dropout_rate,training=tf.convert_to_tensor(hp.is_training))\n",
    "                self.dec = decoder(self.enc,self.dec,hp.num_layers,hp,\"decoder\",seq_len)\n",
    "                \n",
    "                # Final linear projection\n",
    "                self.logits = tf.layers.dense(self.dec, len(en2idx)) #[N,T,len(en2idx)]\n",
    "                self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1),name=\"preds\") #[N,T]\n",
    "            elif hp.beam_width>0:\n",
    "                self.logits = beamDecoder(self.enc,self.decoder_inputs,hp.num_layers,hp,seq_len,scope=\"beam_decoder\")\n",
    "#                 self.logits = tf.layers.dense(self.dec, len(en2idx)) #[N,T,len(en2idx)]\n",
    "                self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1),name=\"preds\") #[N,T]\n",
    "                \n",
    "            if hp.is_training:\n",
    "                #同时把label（即self.y）中所有id不为0（即是真实的word，不是pad）的位置的值用float型的1.0代替作为self.istarget，其shape为[N,T]\n",
    "                self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "                #当self.preds和self.y中对应位置值相等时转为float 1.0,否则为0\n",
    "                self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "\n",
    "                #Loss\n",
    "                self.y_smoothed = label_smoothing(tf.cast(tf.one_hot(self.y,depth=len(en2idx)),dtype=tf.float32))\n",
    "                self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.y_smoothed)\n",
    "                self.mean_loss=tf.reduce_sum(self.loss*self.istarget)/(tf.reduce_sum(self.istarget))\n",
    "\n",
    "                #Training Scheme\n",
    "                self.global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr,beta1=0.9,beta2=0.98,epsilon=1e-8,name='optimizer')\n",
    "                self.train_op= self.optimizer.minimize(self.loss,global_step=self.global_step,name='train_op')\n",
    "\n",
    "                #Summary\n",
    "                tf.summary.scalar('mean_loss',self.mean_loss)\n",
    "                self.merged = tf.summary.merge_all()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Beam(object):\n",
    "    ''' Store the neccesary info for beam search. '''\n",
    "\n",
    "    def __init__(self, size, cuda=False):\n",
    "\n",
    "        self.size = size\n",
    "        self.done = False\n",
    "\n",
    "        self.tt = torch.cuda if cuda else torch\n",
    "\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = self.tt.FloatTensor(size).zero_()\n",
    "        self.all_scores = []\n",
    "\n",
    "        # The backpointers at each time-step.\n",
    "        self.prev_ks = []\n",
    "\n",
    "        # The outputs at each time-step.\n",
    "        self.next_ys = [self.tt.LongTensor(size).fill_(Constants.PAD)]\n",
    "        self.next_ys[0][0] = Constants.BOS\n",
    "\n",
    "    def get_current_state(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        return self.get_tentative_hypothesis()\n",
    "\n",
    "    def get_current_origin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prev_ks[-1]\n",
    "\n",
    "    def advance(self, word_lk):\n",
    "        \"Update the status and check for finished or not.\"\n",
    "        num_words = word_lk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prev_ks) > 0:\n",
    "            beam_lk = word_lk + self.scores.unsqueeze(1).expand_as(word_lk)\n",
    "        else:\n",
    "            beam_lk = word_lk[0]\n",
    "\n",
    "        flat_beam_lk = beam_lk.view(-1)\n",
    "\n",
    "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 1st sort\n",
    "        best_scores, best_scores_id = flat_beam_lk.topk(self.size, 0, True, True) # 2nd sort\n",
    "\n",
    "        self.all_scores.append(self.scores)\n",
    "        self.scores = best_scores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prev_k = best_scores_id / num_words\n",
    "        self.prev_ks.append(prev_k)\n",
    "        self.next_ys.append(best_scores_id - prev_k * num_words)\n",
    "\n",
    "        # End condition is when top-of-beam is EOS.\n",
    "        if self.next_ys[-1][0] == Constants.EOS:\n",
    "            self.done = True\n",
    "            self.all_scores.append(self.scores)\n",
    "\n",
    "        return self.done\n",
    "\n",
    "    def sort_scores(self):\n",
    "        \"Sort the scores.\"\n",
    "        return torch.sort(self.scores, 0, True)\n",
    "\n",
    "    def get_the_best_score_and_idx(self):\n",
    "        \"Get the score of the best in the beam.\"\n",
    "        scores, ids = self.sort_scores()\n",
    "        return scores[1], ids[1]\n",
    "\n",
    "    def get_tentative_hypothesis(self):\n",
    "        \"Get the decoded sequence for the current timestep.\"\n",
    "        if len(self.next_ys) == 1:\n",
    "            dec_seq = self.next_ys[0].unsqueeze(1)\n",
    "        else:\n",
    "            _, keys = self.sort_scores()\n",
    "            hyps = [self.get_hypothesis(k) for k in keys]\n",
    "            hyps = [[Constants.BOS] + h for h in hyps]\n",
    "            dec_seq = torch.from_numpy(np.array(hyps))\n",
    "        return dec_seq\n",
    "\n",
    "    def get_hypothesis(self, k):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        Parameters.\n",
    "             * `k` - the position in the beam to construct.\n",
    "         Returns.\n",
    "\n",
    "            1. The hypothesis\n",
    "            2. The attention at each time step.\n",
    "        \"\"\"\n",
    "        hyp = []\n",
    "        for j in range(len(self.prev_ks) - 1, -1, -1):\n",
    "            hyp.append(self.next_ys[j+1][k])\n",
    "            k = self.prev_ks[j][k]\n",
    "        return hyp[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load vocabulary    \n",
    "#     de2idx, idx2de = load_de_vocab()\n",
    "#     en2idx, idx2en = load_en_vocab()\n",
    "    model=transformer()\n",
    "    sv=tf.train.Supervisor(graph=model.graph,logdir=hp.logdir,save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        for epoch in range(1, hp.num_epochs+1):\n",
    "            if sv.should_stop(): break\n",
    "            for step in tqdm(range(hp.batch_size), total=hp.batch_size, ncols=70, leave=False, unit='b'):\n",
    "                sess.run(model.train_op)\n",
    "            gs = sess.run(model.global_step)\n",
    "            sv.saver.save(sess, hp.logdir + '/model_epoch_%02d_gs_%d' % (epoch, gs))\n",
    "            print(\"save done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 3, 3) dtype=int32_ref>\n",
      "INFO:tensorflow:Restoring parameters from logdir/model_epoch_01_gs_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key decoder_layer_0/Variable/optimizer not found in checkpoint\n\t [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    952\u001b[0m           \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m           start_standard_services=start_standard_services)\n\u001b[0m\u001b[1;32m    954\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mprepare_or_wait_for_session\u001b[0;34m(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)\u001b[0m\n\u001b[1;32m    707\u001b[0m           \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m           init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n\u001b[0m\u001b[1;32m    709\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Loads the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover_last_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1665\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1666\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1667\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key decoder_layer_0/Variable/optimizer not found in checkpoint\n\t [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_6', defined at:\n  File \"/home/hadoop/miniconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/hadoop/miniconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-0c9829cdfaaa>\", line 1, in <module>\n    train()\n  File \"<ipython-input-8-871d7899e260>\", line 6, in train\n    sv=tf.train.Supervisor(graph=model.graph,logdir=hp.logdir,save_model_secs=0)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\", line 300, in __init__\n    self._init_saver(saver=saver)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\", line 448, in _init_saver\n    saver = saver_mod.Saver()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\n    self.build()\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1227, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1263, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 751, in _build_internal\n    restore_sequentially, reshape)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/hadoop/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key decoder_layer_0/Variable/optimizer not found in checkpoint\n\t [[Node: save/RestoreV2_6 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_6/tensor_names, save/RestoreV2_6/shape_and_slices)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0c9829cdfaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-871d7899e260>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervisor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_model_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanaged_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mrequest_stop\u001b[0;34m(self, ex)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mcorresponding\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrecorded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mraised\u001b[0m \u001b[0;32mfrom\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \"\"\"\n\u001b[0;32m--> 812\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mrequest_stop\u001b[0;34m(self, ex)\u001b[0m\n\u001b[1;32m    220\u001b[0m             logging.info(\"Error reported to Coordinator: %s, %s\",\n\u001b[1;32m    221\u001b[0m                          \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                          compat.as_str_any(ex))\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m           \u001b[0;31m# self._exc_info_to_raise should contain a tuple containing exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_str_any\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m       output = [\"%s\\n\\nCaused by op %r, defined at:\\n\" % (self.message,\n\u001b[1;32m     87\u001b[0m                                                           self._op.name,)]\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mcurr_traceback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_traceback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mtraceback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1974\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1975\u001b[0m     \u001b[0;34m\"\"\"Returns the call stack from when this operation was constructed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_convert_stack\u001b[0;34m(self, stack, include_func_start_lineno)\u001b[0m\n\u001b[1;32m   2517\u001b[0m          unused_frame_info) in stack:\n\u001b[1;32m   2518\u001b[0m       \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m       \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetline\u001b[0;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlineno\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mgetlines\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdatecache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_globals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMemoryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mclearcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/linecache.py\u001b[0m in \u001b[0;36mupdatecache\u001b[0;34m(filename, module_globals)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_buffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mdetect_encoding\u001b[0;34m(readline)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOM_UTF8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mbom_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36mread_or_stop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_or_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()\n",
    "# eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(): \n",
    "    def translate_batch(model,src_batch):\n",
    "        model.\n",
    "    # Load graph\n",
    "    hp.is_training=False\n",
    "    model=transformer()\n",
    "    print(\"Graph loaded\")\n",
    "    \n",
    "    # Load data\n",
    "    X, Sources, Targets = load_test_data()\n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "     \n",
    "#     X, Sources, Targets = X[:33], Sources[:33], Targets[:33]\n",
    "     \n",
    "    # Start session \n",
    "    with model.graph.as_default():    \n",
    "        sv = tf.train.Supervisor()\n",
    "        with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            ## Restore parameters\n",
    "            sv.saver.restore(sess, tf.train.latest_checkpoint(hp.logdir))\n",
    "            print(\"Restored!\")\n",
    "              \n",
    "            ## Get model name\n",
    "            mname = open(hp.logdir + '/checkpoint', 'r').read().split('\"')[1] # model name\n",
    "             \n",
    "            ## Inference\n",
    "            if not os.path.exists('results'): os.mkdir('results')\n",
    "            with codecs.open(\"results/\" + mname, \"w\", \"utf-8\") as fout:\n",
    "                list_of_refs, hypotheses = [], []\n",
    "                for i in range(len(X) // hp.batch_size):\n",
    "                    ### Get mini-batches\n",
    "                    x = X[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    sources = Sources[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    targets = Targets[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    ### Autoregressive inference\n",
    "                    preds = np.zeros((hp.batch_size, hp.maxlen), np.int32)\n",
    "                    for j in range(hp.maxlen):\n",
    "                        _preds = sess.run(model.preds, {model.x: x, model.y: preds})\n",
    "                        preds[:, j] = _preds[:, j]\n",
    "                     \n",
    "                    ### Write to file\n",
    "                    for source, target, pred in zip(sources, targets, preds): # sentence-wise\n",
    "                        got = \" \".join(idx2en[idx] for idx in pred).split(\"</S>\")[0].strip()\n",
    "                        fout.write(\"- source: \" + source +\"\\n\")\n",
    "                        fout.write(\"- expected: \" + target + \"\\n\")\n",
    "                        fout.write(\"- got: \" + got + \"\\n\\n\")\n",
    "                        fout.flush()\n",
    "                          \n",
    "                        # bleu score\n",
    "                        ref = target.split()\n",
    "                        hypothesis = got.split()\n",
    "                        if len(ref) > 3 and len(hypothesis) > 3:\n",
    "                            list_of_refs.append([ref])\n",
    "                            hypotheses.append(hypothesis)\n",
    "              \n",
    "                #Calculate bleu score\n",
    "                score = corpus_bleu(list_of_refs, hypotheses)\n",
    "                fout.write(\"Bleu Score = \" + str(100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(): \n",
    "    # Load graph\n",
    "    model=transformer()\n",
    "    hp.is_training=False\n",
    "    print(\"Graph loaded\")\n",
    "    \n",
    "    # Load data\n",
    "    X, Sources, Targets = load_test_data()\n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "     \n",
    "#     X, Sources, Targets = X[:33], Sources[:33], Targets[:33]\n",
    "     \n",
    "    # Start session \n",
    "    with model.graph.as_default():    \n",
    "        sv = tf.train.Supervisor()\n",
    "        with sv.managed_session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "            ## Restore parameters\n",
    "            sv.saver.restore(sess, tf.train.latest_checkpoint(hp.logdir))\n",
    "            print(\"Restored!\")\n",
    "              \n",
    "            ## Get model name\n",
    "            mname = open(hp.logdir + '/checkpoint', 'r').read().split('\"')[1] # model name\n",
    "             \n",
    "            ## Inference\n",
    "            if not os.path.exists('results'): os.mkdir('results')\n",
    "            with codecs.open(\"results/\" + mname, \"w\", \"utf-8\") as fout:\n",
    "                list_of_refs, hypotheses = [], []\n",
    "                for i in range(len(X) // hp.batch_size):\n",
    "                    \n",
    "                    ### Get mini-batches\n",
    "                    x = X[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    sources = Sources[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    targets = Targets[i*hp.batch_size: (i+1)*hp.batch_size]\n",
    "                    ### Autoregressive inference\n",
    "                    preds = np.zeros((hp.batch_size, hp.maxlen), np.int32)\n",
    "                    for j in range(hp.maxlen):\n",
    "                        _preds = sess.run(model.preds, {model.x: x, model.y: preds})\n",
    "                        preds[:, j] = _preds[:, j]\n",
    "                    \n",
    "                    ### Write to file\n",
    "                    for source, target, pred in zip(sources, targets, preds): # sentence-wise\n",
    "                        got = \" \".join(idx2en[idx] for idx in pred).split(\"</S>\")[0].strip()\n",
    "                        fout.write(\"- source: \" + source +\"\\n\")\n",
    "                        fout.write(\"- expected: \" + target + \"\\n\")\n",
    "                        fout.write(\"- got: \" + got + \"\\n\\n\")\n",
    "                        fout.flush()\n",
    "                          \n",
    "                        # bleu score\n",
    "                        ref = target.split()\n",
    "                        hypothesis = got.split()\n",
    "                        if len(ref) > 3 and len(hypothesis) > 3:\n",
    "                            list_of_refs.append([ref])\n",
    "                            hypotheses.append(hypothesis)\n",
    "              \n",
    "                #Calculate bleu score\n",
    "                score = corpus_bleu(list_of_refs, hypotheses)\n",
    "                fout.write(\"Bleu Score = \" + str(100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
