{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bojone/attention/blob/master/attention_tf.py\n",
    "# https://github.com/Kyubyong/transformer/blob/master/modules.py\n",
    "# https://github.com/gxsaccount/attention-is-all-you-need-pytorch/tree/master/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc7436472dcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m             raise TypeError('The value of a feed cannot be a tf.Tensor object. '\n\u001b[0m\u001b[1;32m   1071\u001b[0m                             \u001b[0;34m'Acceptable feed values include Python scalars, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m                             'strings, lists, numpy ndarrays, or TensorHandles.')\n",
      "\u001b[0;31mTypeError\u001b[0m: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles."
     ]
    }
   ],
   "source": [
    "a=tf.placeholder(shape=[None,10],dtype=tf.int32,name=\"t1\")\n",
    "b=tf.placeholder(shape=[None,10],dtype=tf.int32,name=\"t2\")\n",
    "a1=a\n",
    "b1=b\n",
    "c=a1+b1\n",
    "with tf.Session().as_default() as sess:\n",
    "    t3=tf.convert_to_tensor([[1,1,1,1,1,1,1,1,1,1]])\n",
    "    a=t3\n",
    "    sess.run(c,{a:t3,b:[[0,0,0,0,0,0,0,0,0,0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'a_2:0' shape=(10, 40) dtype=float32_ref>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.Variable(tf.truncated_normal([10, 40]),name=\"a\")  # <tf.Variable 'a:0' shape=(2, 5) dtype=float32_ref>\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "#     sess_run_a = sess.run(get_variable_a)\n",
    "    print (a)\n",
    "    shape=a.shape\n",
    "    print(shape[0])\n",
    "    '''\n",
    "    [[-0.82719386 -0.73760307  0.73487639 -0.56606793  0.80296755]\n",
    "    [ 0.19305325 -0.55414021  0.03531528 -0.75267315  0.73095655]]\n",
    "    <type 'numpy.ndarray'>\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1. -1. -1.]\n",
      "  [ 1.  1.  1.]]\n",
      "\n",
      " [[ 1.  1.  2.]\n",
      "  [ 3.  3.  3.]]]\n",
      "[[[1. 0. 0.]\n",
      "  [1. 1. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 1. 0.]]]\n",
      "[[[-4.2949673e+09 -4.2949673e+09 -4.2949673e+09]\n",
      "  [-4.2949673e+09 -4.2949673e+09 -4.2949673e+09]]\n",
      "\n",
      " [[-4.2949673e+09 -4.2949673e+09 -4.2949673e+09]\n",
      "  [-4.2949673e+09 -4.2949673e+09 -4.2949673e+09]]]\n",
      "[[[-1.0000000e+00 -4.2949673e+09 -4.2949673e+09]\n",
      "  [ 1.0000000e+00  1.0000000e+00 -4.2949673e+09]]\n",
      "\n",
      " [[ 1.0000000e+00 -4.2949673e+09 -4.2949673e+09]\n",
      "  [ 3.0000000e+00  3.0000000e+00 -4.2949673e+09]]]\n",
      "[[[1.  0.  0. ]\n",
      "  [0.5 0.5 0. ]]\n",
      "\n",
      " [[1.  0.  0. ]\n",
      "  [0.5 0.5 0. ]]]\n"
     ]
    }
   ],
   "source": [
    "k=[[[-1.,-1.,-1.],[1.,1.,1.]],[[1.,1.,2.],[3.,3.,3.]]]\n",
    "key=tf.Variable(k)\n",
    "mask=tf.sign(tf.abs(tf.reduce_sum(key,axis=-1)))\n",
    "# mask=tf.tile(mask,[8,1])\n",
    "# mask=tf.tile(tf.expand_dims(mask, 1), [1, 3, 1])\n",
    "\n",
    "# mask = tf.cast(tf.sequence_mask(1), tf.float32)\n",
    "init=tf.global_variables_initializer()\n",
    "# with tf.variable_scope(\"1\",reuse=tf.AUTO_REUSE) as scope:\n",
    "# Weights1 = tf.get_variable('Weights', (2,3))\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "#     l=tf.nn.relu(key)\n",
    "#     print(tf.abs(tf.reduce_sum(key,axis=-1)).eval())\n",
    "#     print(key.eval())\n",
    "#     print(key.eval())\n",
    "    A=key\n",
    "#     diag_vals = tf.ones_like(A[0, :, :]) # (T_q, T_k)\n",
    "# #     print(diag_vals.eval())\n",
    "#     tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "#     print(tril.eval())\n",
    "#     masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(A)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "#     print()\n",
    "#     print(masks.eval())\n",
    "#     paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "#     print(paddings.eval())\n",
    "#     print()\n",
    "#     outputs = tf.where(tf.equal(masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "#     print(outputs.eval())\n",
    "# #         Key Masking\n",
    "#     key_masks = tf.sign(tf.abs(tf.reduce_sum(A, axis=-1))) # (N, T_k)\n",
    "#     key_masks = tf.tile(key_masks, [1, 1]) # (h*N, T_k)\n",
    "#     key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(A)[1], 1]) # (h*N, T_q, T_k)\n",
    "    \n",
    "#     paddings = tf.ones_like(A)*(-2**32+1)\n",
    "#     A = tf.where(tf.equal(key_masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "#     print(paddings.eval())\n",
    "#     print(A.eval())\n",
    "#     print(tf.concat((tf.ones_like(A[:, :1])*2, A[:, :-1]), -1).eval()) # 2:<S>\n",
    "#     sess.run(Weights1)\n",
    "#     print(Weights1.eval())\n",
    "    print(A.eval())\n",
    "    diag_vals = tf.ones_like(A[0, :, :]) # (T_q, T_k)\n",
    "    \n",
    "    tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "    masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(A)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "    print(masks.eval())\n",
    "    paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "    print(paddings.eval())\n",
    "    A = tf.where(tf.equal(masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "    print(outputs.eval())\n",
    "    A = tf.nn.softmax(A)\n",
    "    print(A.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(query,key,value,scope,seq_len,units_per_head=None,num_heads=8,mask=False):\n",
    "    #使用时一般K和V一样\n",
    "    #Q(N,Tq,C) K=V(N,Tk,C)\n",
    "    Q=tf.layers.dense(query,units_per_head*num_units,activation=tf.nn.relu)\n",
    "    K=tf.layers.dense(key,units_per_head*num_units,activation=tf.nn.relu)\n",
    "    V=tf.layers.dense(value,units_per_head*num_units,activation=tf.nn.relu)\n",
    "    #tf.split(dimension, num_split, input)：dimension的意思就是输入张量的哪一个维度，\n",
    "    #如果是0就表示对第0维度进行切割。num_split就是切割的数量\n",
    "    Q=tf.concat(tf.split(Q,num_heads,axis=2),axis=0)#(h*N,Tq,C/h)\n",
    "    K=tf.concat(tf.split(K,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "    V=tf.concat(tf.split(V,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "    #计算内积，然后mask，然后softmax\n",
    "    A = tf.matmul(Q,tf.transpose(K,[0,2,1]))#(h*N,Tq,Tk)\n",
    "    A = A /tf.sqrt(float(K.shape[-1]))##\n",
    "    \n",
    "    \n",
    "    # Key Masking\n",
    "    #将元素中最后一个为度之和全为0的位置标记为0 source(N, T_k,word_dim)\n",
    "    key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1))) # (N, T_k)\n",
    "    key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "    #由于每个queries都要对应这些keys，而mask的key对每个queries都是mask的\n",
    "    key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1]) # (h*N, T_q, T_k)\n",
    "    #定义一个和outputs同shape的paddings，该tensor每个值都设定的极小。用where函数比较，当对应位置的key_masks值为0\n",
    "    #也就是需要mask时，outputs的该值（attention score）设置为极小的值（利用paddings实现），否则保留原来的outputs值。 \n",
    "    paddings = tf.ones_like(A)*(-2**32+1)\n",
    "    A = tf.where(tf.equal(key_masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "    \n",
    "    #Causality Future blinding\n",
    "    if mask:\n",
    "        #首先定义一个和outputs后两维的shape相同shape（T_q,T_k）的一个张量（矩阵）。\n",
    "        #然后将该矩阵转为三角阵tril。三角阵中，对于每一个T_q,凡是那些大于它角标的T_k值全都为0，\n",
    "        #这样作为mask就可以让query只取它之前的key（self attention中query即key）。\n",
    "        #由于该规律适用于所有query，接下来仍用tile扩展堆叠其第一个维度，构成masks，shape为(h*N, T_q,T_k).\n",
    "        diag_vals = tf.ones_like(A[0, :, :]) # (T_q, T_k)\n",
    "        tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(A)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "\n",
    "        paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "        outputs = tf.where(tf.equal(masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "    A = tf.nn.softmax(A)# (h*N, T_q, T_k)\n",
    "    \n",
    "    # Query Masking\n",
    "    query_masks = tf.sign(tf.abs(tf.reduce_sum(queries, axis=-1))) # (N, T_q)\n",
    "    query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "    query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(keys)[1]]) # (h*N, T_q, T_k)\n",
    "    \n",
    "    A *= query_masks # broadcasting. (N, T_q, C)\n",
    "    A=tf.layers.dropout(outputs, rate=dropout_rate)\n",
    "    \n",
    "    # weighted sum\n",
    "    outputs=tf.matmul(A,V)# ( h*N, T_q, C/h)\n",
    "    \n",
    "    #Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "    \n",
    "    # Residual connection\n",
    "    outputs += queries\n",
    "    \n",
    "    #Normalize\n",
    "    outputs=layer_normal(outputs)\n",
    "    \n",
    "    return outputs\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs,num_units=[2048,512],reuse=None):\n",
    "    outputs= tf.layers.conv1d(\n",
    "        inputs,filters=num_units[0],kernel_size=1,activation=tf.nn.relu,use_bias=True)\n",
    "    outputs=tf.layers.conv1d(\n",
    "        inputs,filters=num_units[0],kernel_size=1,activation=None,use_bias=True)\n",
    "    outputs=layer_normal(outputs+inputs)\n",
    "def layer_normal(inputs,epsilon = 1e-8):\n",
    "    mean,variance = tf.nn.moments(inputs,[-1],keep_dims=True)\n",
    "    beta= tf.Variable(tf.zeros(params_shape))\n",
    "    gamma = tf.Variable(tf.ones(params_shape))\n",
    "    normalized = (inputs-mean)/(((variance + epsilon) ** (.5)))\n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subLayer():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(inputs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder (inputs,num_layers=6,hparams):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"encoder layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            self.enc = multihead_attention(\n",
    "                query=self.enc,\n",
    "                key=self.enc,\n",
    "                value=self.enc,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=False,\n",
    "                seq_len)\n",
    "            self.enc = feedforward(self.enc, num_units=[4*hparams.hidden_units, hparams.hidden_units])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder (inputs,num_layers=6,hparams,scope):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"encoder layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            self.dec=multihead_attention(\n",
    "                    query=self.dec,\n",
    "                    key=self.dec,\n",
    "                    value=self.dec,\n",
    "                    units_per_head=hparams.units_per_head,\n",
    "                    num_heads=hparams.num_heads,\n",
    "                    mask=False,\n",
    "                    seq_len)\n",
    "            self.dec=multihead_attention(\n",
    "                    query=self.dec,\n",
    "                    key=self.enc,\n",
    "                    value=self.enc,\n",
    "                    units_per_head=hparams.units_per_head,\n",
    "                    num_heads=hparams.num_heads,\n",
    "                    mask=False,\n",
    "                    seq_len)\n",
    "            self.dec = feedforward(self.dec, num_units=[4*hp.hidden_units, hp.hidden_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(self, x, hidden_size, num_blocks=4, num_heads=8, activation=tf.nn.relu):\n",
    "    for i in range(num_blocks):\n",
    "        with tf.variable_scope(\"num_blocks_{}\".format(i)):\n",
    "            # Multihead Attention\n",
    "            #用self.y来初始化解码器的输入。decoder_inputs和self.y相比，\n",
    "            #去掉了最后一个句子结束符，而在每句话最前面加了一个初始化为2的id，\n",
    "            #即<S> ，代表开始。shape和self.y一样为[N,T]。\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_beam_search(update_fn, initial_state, sequence_length, beam_width,\n",
    "                    begin_token_id, end_token_id, name=\"rnn\"):\n",
    "    \"\"\"Beam-search decoder for recurrent models.\n",
    "\n",
    "    Args:\n",
    "    update_fn: Function to compute the next state and logits given the current\n",
    "               state and ids.\n",
    "    initial_state: Recurrent model states.\n",
    "    sequence_length: Length of the generated sequence.\n",
    "    beam_width: Beam width.\n",
    "    begin_token_id: Begin token id.\n",
    "    end_token_id: End token id.\n",
    "    name: Scope of the variables.\n",
    "    Returns:\n",
    "    ids: Output indices.\n",
    "    logprobs: Output log probabilities probabilities.\n",
    "    \"\"\"\n",
    "    batch_size = initial_state.shape.as_list()[0]\n",
    "\n",
    "    state = tf.tile(tf.expand_dims(initial_state, axis=1), [1, beam_width, 1])\n",
    "\n",
    "    sel_sum_logprobs = tf.log([[1.] + [0.] * (beam_width - 1)])\n",
    "\n",
    "    ids = tf.tile([[begin_token_id]], [batch_size, beam_width])\n",
    "    sel_ids = tf.zeros([batch_size, beam_width, 0], dtype=ids.dtype)\n",
    "\n",
    "    mask = tf.ones([batch_size, beam_width], dtype=tf.float32)\n",
    "\n",
    "    for i in range(sequence_length):\n",
    "    with tf.variable_scope(name, reuse=True if i > 0 else None):\n",
    "\n",
    "        state, logits = update_fn(state, ids)\n",
    "        logits = tf.nn.log_softmax(logits)\n",
    "\n",
    "        sum_logprobs = (\n",
    "          tf.expand_dims(sel_sum_logprobs, axis=2) +\n",
    "          (logits * tf.expand_dims(mask, axis=2)))\n",
    "\n",
    "        num_classes = logits.shape.as_list()[-1]\n",
    "\n",
    "        sel_sum_logprobs, indices = tf.nn.top_k(\n",
    "          tf.reshape(sum_logprobs, [batch_size, num_classes * beam_width]),\n",
    "          k=beam_width)\n",
    "\n",
    "        ids = indices % num_classes\n",
    "\n",
    "        beam_ids = indices // num_classes\n",
    "\n",
    "        state = batch_gather(state, beam_ids)\n",
    "\n",
    "        sel_ids = tf.concat([batch_gather(sel_ids, beam_ids),\n",
    "                           tf.expand_dims(ids, axis=2)], axis=2)\n",
    "\n",
    "        mask = (batch_gather(mask, beam_ids) *\n",
    "              tf.to_float(tf.not_equal(ids, end_token_id)))\n",
    "\n",
    "    return sel_ids, sel_sum_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun1(model):\n",
    "    model.test=1\n",
    "class model:\n",
    "    test=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "model=model()\n",
    "fun1(model)\n",
    "print(model.test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
