{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../transformer/\")\n",
    "from tqdm import tqdm\n",
    "from data_load import get_batch_data, load_de_vocab, load_en_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams(object):\n",
    "    #data\n",
    "    source_train = 'corpora/train.tags.de-en.de'\n",
    "    target_train = 'corpora/train.tags.de-en.en'\n",
    "    source_test = 'corpora/IWSLT16.TED.tst2014.de-en.de.xml'\n",
    "    target_test = 'corpora/IWSLT16.TED.tst2014.de-en.en.xml'\n",
    "    \n",
    "    # training\n",
    "    batch_size=32\n",
    "    lr = 0.001\n",
    "    logdir = 'logdir' # log directory\n",
    "    #model\n",
    "    maxlen = 10 # Maximum number of words in a sentence. alias = T.\n",
    "                # Feel free to increase this if you are ambitious.\n",
    "    min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n",
    "    hidden_units = 512 # alias = C\n",
    "    num_blocks = 6 # number of encoder/decoder blocks\n",
    "    num_epochs = 1\n",
    "    num_heads = 8\n",
    "    units_per_head=1024\n",
    "    dropout_rate = 0.1\n",
    "    is_training=True\n",
    "    num_layers=6\n",
    "hp=Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(query,key,value,seq_len,units_per_head=1024,num_heads=8,mask=False,scope=\"multihead_attention\", \n",
    "reuse=None):\n",
    "    #使用时一般K和V一样\n",
    "    #query(N,Tq/h,C) key=value(N,Tk/h,C)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "#         units_per_head = query.shape[-1]\n",
    "        num_units = query.shape[-1]\n",
    "        Q=tf.layers.dense(query,num_units,activation=tf.nn.relu)# (N, T_q, C)\n",
    "        K=tf.layers.dense(key,num_units,activation=tf.nn.relu)# (N, T_k, C)\n",
    "        V=tf.layers.dense(value,num_units,activation=tf.nn.relu)# (N, T_k, C)\n",
    "        #tf.split(dimension, num_split, input)：dimension的意思就是输入张量的哪一个维度，\n",
    "        #如果是0就表示对第0维度进行切割。num_split就是切割的数量\n",
    "        Q=tf.concat(tf.split(Q,num_heads,axis=2),axis=0)#(h*N,Tq,C/h)\n",
    "        K=tf.concat(tf.split(K,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "        V=tf.concat(tf.split(V,num_heads,axis=2),axis=0)#(h*N,Tk,C/h)\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = tf.matmul(Q,tf.transpose(K,[0,2,1]))#(h*N,Tq,Tk)\n",
    "        A = A /(K.get_shape().as_list()[-1] ** 0.5)\n",
    "\n",
    "        # Key Masking\n",
    "        #将元素中最后一个为度之和全为0的位置标记为0 source(N, T_k,word_dim)\n",
    "        key_masks = tf.sign(tf.abs(tf.reduce_sum(key, axis=-1))) # (N, T_k)\n",
    "        key_masks = tf.tile(key_masks, [num_heads, 1]) # (h*N, T_k)\n",
    "        #由于每个queries都要对应这些keys，而mask的key对每个queries都是mask的\n",
    "        key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, Q.shape[1], 1]) # (h*N, T_q, T_k)\n",
    "        #定义一个和outputs同shape的paddings，该tensor每个值都设定的极小。用where函数比较，当对应位置的key_masks值为0\n",
    "        #也就是需要mask时，outputs的该值（attention score）设置为极小的值（利用paddings实现），否则保留原来的outputs值。\n",
    "        paddings = tf.ones_like(A)*(-2**32+1)\n",
    "        A = tf.where(tf.equal(key_masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "\n",
    "        #Causality Future blinding\n",
    "        if mask:\n",
    "            #首先定义一个和outputs后两维的shape相同shape（T_q,T_k）的一个张量（矩阵）。\n",
    "            #然后将该矩阵转为三角阵tril。三角阵中，对于每一个T_q,凡是那些大于它角标的T_k值全都为0，\n",
    "            #这样作为mask就可以让query只取它之前的key（self attention中query即key）。\n",
    "            #由于该规律适用于所有query，接下来仍用tile扩展堆叠其第一个维度，构成masks，shape为(h*N, T_q,T_k).\n",
    "            diag_vals = tf.ones_like(A[0, :, :]) # (T_q, T_k)\n",
    "            tril = tf.contrib.linalg.LinearOperatorTriL(diag_vals).to_dense() # (T_q, T_k)\n",
    "            masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(A)[0], 1, 1]) # (h*N, T_q, T_k)\n",
    "            paddings = tf.ones_like(masks)*(-2**32+1)\n",
    "            A = tf.where(tf.equal(masks, 0), paddings, A) # (h*N, T_q, T_k)\n",
    "        A = tf.nn.softmax(A)# (h*N, T_q, T_k)\n",
    "\n",
    "        # Query Masking\n",
    "        query_masks = tf.sign(tf.abs(tf.reduce_sum(query, axis=-1))) # (N, T_q)\n",
    "        query_masks = tf.tile(query_masks, [num_heads, 1]) # (h*N, T_q)\n",
    "        query_masks = tf.tile(tf.expand_dims(query_masks, -1), [1, 1, tf.shape(key)[1]]) # (h*N, T_q, T_k)\n",
    "\n",
    "        A *= query_masks # broadcasting. (N, T_q, C)\n",
    "        A=tf.layers.dropout(A, rate=hp.dropout_rate)\n",
    "\n",
    "        # weighted sum\n",
    "        outputs=tf.matmul(A,V)# ( h*N, T_q, C/h)\n",
    "\n",
    "        #Restore shape\n",
    "        outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, C)\n",
    "\n",
    "        # Residual connection\n",
    "        outputs += query\n",
    "\n",
    "        #Normalize\n",
    "        outputs=layer_normal(outputs)\n",
    "    return outputs\n",
    "\n",
    "def feedforward(inputs,num_units=[2048,512],reuse=None):\n",
    "    outputs= tf.layers.conv1d(\n",
    "        inputs,filters=num_units[0],kernel_size=1,activation=tf.nn.relu,use_bias=True)\n",
    "    outputs=tf.layers.conv1d(\n",
    "        outputs,filters=num_units[1],kernel_size=1,activation=None,use_bias=True)\n",
    "    # Residual connection\n",
    "    outputs += inputs\n",
    "    outputs=layer_normal(outputs)\n",
    "    return outputs\n",
    "def layer_normal(inputs,epsilon = 1e-8):\n",
    "    mean,variance = tf.nn.moments(inputs,[-1],keep_dims=True)\n",
    "    params_shape = inputs.shape[-1:]\n",
    "    beta= tf.Variable(tf.zeros(params_shape))\n",
    "    gamma = tf.Variable(tf.ones(params_shape))\n",
    "    normalized = (inputs-mean)/(((variance + epsilon) ** (.5)))\n",
    "    outputs = gamma * normalized + beta\n",
    "    return outputs\n",
    "def encoder (model,num_layers,hparams,scope,seq_len):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"encoder_layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            model.enc = multihead_attention(\n",
    "                query=model.enc,\n",
    "                key=model.enc,\n",
    "                value=model.enc,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=False,\n",
    "                seq_len=seq_len)\n",
    "            model.enc = feedforward(model.enc, num_units=[4*hparams.hidden_units, hparams.hidden_units])\n",
    "    return model.enc\n",
    "def decoder (model,num_layers,hparams,scope,seq_len):\n",
    "    for i in range(num_layers):\n",
    "        scope=\"decoder_layer_{}\".format(i)\n",
    "        with tf.variable_scope(scope):\n",
    "            model.dec=multihead_attention(\n",
    "                query=model.dec,\n",
    "                key=model.dec,\n",
    "                value=model.dec,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=True,\n",
    "                scope=\"self_attention\",\n",
    "                seq_len=seq_len)\n",
    "            model.dec=multihead_attention(\n",
    "                query=model.dec,\n",
    "                key=model.enc,\n",
    "                value=model.enc,\n",
    "                units_per_head=hparams.units_per_head,\n",
    "                num_heads=hparams.num_heads,\n",
    "                mask=False,\n",
    "                scope=\"vanilla_attention\",\n",
    "                seq_len=seq_len)\n",
    "            model.dec = feedforward(model.dec, num_units=[4*hp.hidden_units, hp.hidden_units])\n",
    "    return model.dec\n",
    "def position_embedding(inputs,position_size):\n",
    "    batch_size,seq_len = tf.shape(inputs)[0],tf.shape(inputs)[1]\n",
    "    pos_j = 1. / tf.pow(10000., 2 * tf.range(position_size / 2, dtype=tf.float32) / position_size)\n",
    "    pos_j = tf.expand_dims(pos_j, 0)\n",
    "    pos_i = tf.range(tf.cast(seq_len, tf.float32), dtype=tf.float32)\n",
    "    pos_i = tf.expand_dims(pos_i, 1)\n",
    "    pos_ij = tf.matmul(pos_i, pos_j)\n",
    "    pos_ij = tf.concat([tf.cos(pos_ij), tf.sin(pos_ij)], 1)\n",
    "    position_embedding = tf.expand_dims(pos_ij, 0) + tf.zeros((batch_size, seq_len, position_size))\n",
    "    return position_embedding\n",
    "def word_embedding(inputs,vocab_size,num_units,zero_pad=True,scale=True,scope=\"embedding\",reuse=None):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "    return outputs\n",
    "\n",
    "def label_smoothing(inputs,epsilon=0.1):\n",
    "    \"\"\"Applies label smoothing. See https://arxiv.org/abs/1512.00567.\"\"\"\n",
    "    K = tf.to_float(tf.shape(inputs)[-1]) # number of channels\n",
    "    return ((1.-epsilon)*inputs+(epsilon/K))\n",
    "#     return ((1-epsilon)*inputs+(epsilon/K))\n",
    "\n",
    "# https://github.com/vahidk/EffectiveTensorflow#beam_search\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(object):\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        # Load vocabulary    \n",
    "        de2idx, idx2de = load_de_vocab()\n",
    "        en2idx, idx2en = load_en_vocab()\n",
    "        #[N,T,hp.hidden_units]\n",
    "        self.enc = word_embedding(self.x,vocab_size=len(de2idx),num_units=hp.hidden_units,scale=True,scope=\"enc_embed\")\n",
    "        #[N,T,hp.hidden_units]??\n",
    "        self.enc+= position_embedding(self.x,hp.hidden_units)\n",
    "        self.enc = tf.layers.dropout(self.enc, rate=hp.dropout_rate,training=tf.convert_to_tensor(hp.is_training))\n",
    "        seq_len=hp.maxlen\n",
    "        self.enc = encoder(self,hp.num_layers,hp,\"encoder\",seq_len)\n",
    "        \n",
    "        self.decoder_inputs = tf.concat((tf.ones_like(self.y[:, :1])*2, self.y[:, :-1]), -1) # 2:<S>\n",
    "        self.dec = word_embedding(self.decoder_inputs, vocab_size=len(en2idx), \n",
    "                                  num_units=hp.hidden_units,scale=True,scope=\"dec_embed\")\n",
    "        self.dec+=position_embedding(self.x,hp.hidden_units)\n",
    "        self.dec = tf.layers.dropout(self.dec, rate=hp.dropout_rate,training=tf.convert_to_tensor(hp.is_training))\n",
    "        \n",
    "        self.dec = decoder(self,hp.num_layers,hp,\"decoder\",seq_len)\n",
    "        \n",
    "        \n",
    "        # Final linear projection\n",
    "        self.logits = tf.layers.dense(self.dec, len(en2idx)) #[N,T,len(en2idx)]\n",
    "        self.preds = tf.to_int32(tf.argmax(self.logits, axis=-1)) #[N,T]\n",
    "        #同时把label（即self.y）中所有id不为0（即是真实的word，不是pad）的位置的值用float型的1.0代替作为self.istarget，其shape为[N,T]\n",
    "        self.istarget = tf.to_float(tf.not_equal(self.y, 0))\n",
    "        #当self.preds和self.y中对应位置值相等时转为float 1.0,否则为0\n",
    "        self.acc = tf.reduce_sum(tf.to_float(tf.equal(self.preds, self.y))*self.istarget)/ (tf.reduce_sum(self.istarget))\n",
    "        \n",
    "        if hp.is_training:\n",
    "            #Loss\n",
    "            print(\"11111\")\n",
    "            self.y_smoothed = label_smoothing(tf.cast(tf.one_hot(self.y,depth=len(en2idx)),dtype=tf.float32))\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,labels=self.y_smoothed)\n",
    "            self.mean_loss=tf.reduce_sum(self.loss*self.istarget)/(tf.reduce_sum(self.istarget))\n",
    "            \n",
    "            #Training Scheme\n",
    "            self.global_step=tf.Variable(0,name='global_step',trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=hp.lr,beta1=0.9,beta2=0.98,epsilon=1e-8)\n",
    "            self.train_op= self.optimizer.minimize(self.loss,global_step=self.global_step)\n",
    "            \n",
    "            #Summary\n",
    "            tf.summary.scalar('mean_loss',self.mean_loss)\n",
    "            self.merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Load vocabulary    \n",
    "    de2idx, idx2de = load_de_vocab()\n",
    "    en2idx, idx2en = load_en_vocab()\n",
    "    \n",
    "    # Start session\n",
    "    \n",
    "    print(\"1111\")\n",
    "    with tf.Session() as sess:\n",
    "        if hp.is_training:\n",
    "            x,y,num_batch = get_batch_data() # (N, T)\n",
    "        else: #inference\n",
    "            x = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "            y = tf.placeholder(tf.int32, shape=(None, hp.maxlen))\n",
    "            \n",
    "        model=transformer(x,y)\n",
    "        print(\"2222\")\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(1, hp.num_epochs+1):\n",
    "            for step in tqdm(range(num_batch), total=num_batch, ncols=70, leave=False, unit='b'):\n",
    "                sess.run(model.train_op)\n",
    "                print(model.loss)\n",
    "            gs = sess.run(model.global_step)\n",
    "#         sess.saver.save(sess,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111\n",
      "11111\n",
      "2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                         | 0/1703 [00:00<?, ?b/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
